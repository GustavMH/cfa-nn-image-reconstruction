{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":7726797,"sourceType":"datasetVersion","datasetId":4514338}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport joblib\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\n\n!pip install pytorch-msssim\nfrom pytorch_msssim import ssim\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-16T10:06:23.601991Z","iopub.execute_input":"2024-03-16T10:06:23.602338Z","iopub.status.idle":"2024-03-16T10:06:52.773103Z","shell.execute_reply.started":"2024-03-16T10:06:23.602309Z","shell.execute_reply":"2024-03-16T10:06:52.771551Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorch-msssim\n  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from pytorch-msssim) (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-msssim) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-msssim) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-msssim) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-msssim) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-msssim) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-msssim) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->pytorch-msssim) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->pytorch-msssim) (1.3.0)\nDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\nInstalling collected packages: pytorch-msssim\nSuccessfully installed pytorch-msssim-1.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"def salt_and_pepper(image_tensor, noise_level=0.05):\n    \"\"\"\n    Apply salt and pepper noise to an image tensor.\n    \n    Parameters:\n    - image_tensor (torch.Tensor): A torch tensor representing the image.\n    - noise_level (float): The percentage of image pixels to be affected by noise.\n\n    Returns:\n    - torch.Tensor: Image tensor with salt and pepper noise added.\n    \"\"\"\n    noisy_image = image_tensor.clone()\n    num_noisy_pixels = int(noise_level * image_tensor.nelement())\n    salt_indices = torch.randperm(image_tensor.nelement())[:num_noisy_pixels // 2]\n    noisy_image.view(-1)[salt_indices] = 1.0\n    pepper_indices = torch.randperm(image_tensor.nelement())[:num_noisy_pixels // 2]\n    noisy_image.view(-1)[pepper_indices] = 0.0\n    return noisy_image\n\ndef load_images(directory, target_size=(256, 256), t = \".png\"):\n    \"\"\"\n    Load images from dir\n    \n    Parameters:\n    - directory (str): directory to get data\n    - target_size (tuple): expected size of images without channels\n\n    Returns:\n    - data (torch.Tensor): A torch stack with the data\n    \"\"\"\n    images = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(t):\n                image_path = os.path.join(root, file)\n                image = Image.open(image_path).convert('RGB')\n                transform = transforms.Compose([\n                    transforms.Resize(target_size),\n                    transforms.ToTensor(), \n                ])\n                image_tensor = transform(image)\n\n                if image_tensor.max() > 1:\n                    image_tensor = image_tensor / 255.0\n\n                images.append(image_tensor)\n    return torch.stack(images)\n\ndef tensor_to_image(tensor):\n    \"\"\"\n    Apply to tensor to be print in plt\n    \n    Parameters:\n    - image_tensor (torch.Tensor): A torch tensor representing the image.\n\n    Returns:\n    - numpy (np.array): A np array representing the image.\n    \"\"\"\n    if tensor.max() > 1:\n        tensor = tensor / 255.0\n\n    return tensor.cpu().detach().numpy().transpose(1, 2, 0)\n\nclass PairedDataset(Dataset):\n    def __init__(self, data_clean, data_noisy):\n        self.data_clean = data_clean\n        self.data_noisy = data_noisy\n\n    def __len__(self):\n        # Assuming both datasets have the same length\n        return len(self.data_clean)\n\n    def __getitem__(self, idx):\n        clean_image = self.data_clean[idx]\n        noisy_image = self.data_noisy[idx]\n\n        return noisy_image, clean_image\n\nclass DenoisingAutoencoder(nn.Module):\n    def __init__(self):\n        super(DenoisingAutoencoder, self).__init__()\n        # Encoder\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(64),\n            nn.ReLU(True))\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(128),\n            nn.ReLU(True))\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(256),\n            nn.ReLU(True))\n        self.enc4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(512),\n            nn.ReLU(True))\n        self.enc5 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(512),\n            nn.ReLU(True))\n\n        # Decoder\n        self.dec1 = nn.Sequential(\n            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.Dropout(0.5))\n        self.dec2 = nn.Sequential(\n            nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1), # the 1024 comes from concatenation\n            nn.BatchNorm2d(256),\n            nn.ReLU(True))\n        self.dec3 = nn.Sequential(\n            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(128),\n            nn.ReLU(True))\n        self.dec4 = nn.Sequential(\n            nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(64),\n            nn.ReLU(True))\n        self.dec5 = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1), \n            nn.Tanh())\n\n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(e1)\n        e3 = self.enc3(e2)\n        e4 = self.enc4(e3)\n        e5 = self.enc5(e4)\n\n        # Decoder with skip connections\n        d1 = self.dec1(e5)\n        d1 = torch.cat((d1, e4), dim=1)  # skip connection\n        d2 = self.dec2(d1)\n        d2 = torch.cat((d2, e3), dim=1)  # skip connection\n        d3 = self.dec3(d2)\n        d3 = torch.cat((d3, e2), dim=1)  # skip connection\n        d4 = self.dec4(d3)\n        d4 = torch.cat((d4, e1), dim=1)  # skip connection\n        d5 = self.dec5(d4)\n        return d5","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_norm = load_images(\"/kaggle/input/traffic-signs/GTSRB/train/00001\")\n\ntrain_snp = []\n\nnoise_level = 0.5\nfor image in train_norm:\n    i = salt_and_pepper(image, noise_level=noise_level)\n    train_snp.append(i)\n\ntrain_snp = torch.stack(train_snp)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.imshow(tensor_to_image(data[0]))\nplt.title(f'Data {data[0].dtype}')\nplt.subplot(1, 2, 2)\nplt.imshow(tensor_to_image(data_snp[0]))\nplt.title(f'SNP {data_snp[0].dtype}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\npaired_dataset = PairedDataset(train_norm, train_snp)\npaired_loader = DataLoader(paired_dataset, batch_size=32, shuffle=True)\n\ndef ssim_loss(y_true, y_pred):\n    return 1 - ssim(y_true, y_pred, data_range=1, size_average=True)\n\nmodel = DenoisingAutoencoder().to(device)\ncriterion = ssim_loss\nloss = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training Loop\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for noisy_images, clean_images in paired_loader:\n        noisy_images, clean_images = noisy_images.to(device), clean_images.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(noisy_images)\n        loss = criterion(outputs, clean_images)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    epoch_loss = running_loss / len(paired_loader)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n    \nfor param in model.parameters():\n    if torch.isnan(param).any() or torch.isinf(param).any():\n        print(\"Model contains NaN or inf values\")\n        \njoblib.dump(model, f'/kaggle/celebA/model_{noise_level}.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/model_0.5.pkl', 'rb') as f:\n    model = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_norm = load_images(\"/kaggle/input/traffic-signs/GTSRB/test/00001\")\n\nval_snp = [] \n\nfor image in val:\n    i = salt_and_pepper(image, noise_level=1)\n    val_snp.append(i)\n\nval_snp = torch.stack(val_snp)\n\ntest_loader = DataLoader(val_snp, batch_size=32, shuffle=False)\nmodel.eval()\nmodel.to(device)\nfirst_batch = True\n\nwith torch.no_grad():\n    for inputs in test_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n\n        if first_batch:\n            inputs_cpu = inputs.cpu()\n            outputs_cpu = outputs.cpu()\n            fig, axs = plt.subplots(3, 3, figsize=(15, 6))\n            for i in range(3):\n                # Input images\n                axs[0, i].imshow(val[i].permute(1, 2, 0).squeeze())\n                axs[0, i].title.set_text('Input Image ' + str(i+1))\n                axs[0, i].axis('off')\n                \n                # Input images\n                axs[1, i].imshow(inputs_cpu[i].permute(1, 2, 0).squeeze())\n                axs[1, i].title.set_text('Noisy Image ' + str(i+1))\n                axs[1, i].axis('off')\n                \n                # Output images\n                axs[2, i].imshow(outputs_cpu[i].permute(1, 2, 0).squeeze())\n                axs[2, i].title.set_text('Output Image ' + str(i+1))\n                axs[2, i].axis('off')\n            \n            plt.show()\n            first_batch = False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_norm = val_norm\n\nval_snp = [] \n\nfor image in val_norm:\n    i = salt_and_pepper(image, noise_level=1)\n    val_snp.append(i)\n\nval_snp = torch.stack(val_snp)\n\ntest_loader = DataLoader(val_snp, batch_size=32, shuffle=False)\nmodel.eval()\nmodel.to(device)\nfirst_batch = True\n\nwith torch.no_grad():\n    for inputs in test_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n\n        if first_batch:\n            inputs_cpu = inputs.cpu()\n            outputs_cpu = outputs.cpu()\n            fig, axs = plt.subplots(3, 3, figsize=(15, 6))\n            for i in range(3):\n                # Input images\n                axs[0, i].imshow(val[i].permute(1, 2, 0).squeeze())\n                axs[0, i].title.set_text('Input Image ' + str(i+1))\n                axs[0, i].axis('off')\n                \n                # Input images\n                axs[1, i].imshow(inputs_cpu[i].permute(1, 2, 0).squeeze())\n                axs[1, i].title.set_text('Noisy Image ' + str(i+1))\n                axs[1, i].axis('off')\n                \n                # Output images\n                axs[2, i].imshow(outputs_cpu[i].permute(1, 2, 0).squeeze())\n                axs[2, i].title.set_text('Output Image ' + str(i+1))\n                axs[2, i].axis('off')\n            \n            plt.show()\n            first_batch = False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}