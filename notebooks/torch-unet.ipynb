{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#!pip install pytorch-msssim\n",
    "#from pytorch_msssim import ssim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "\n",
    "from cfa import colorize_cfa, rgb_kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([101, 3, 160, 160]), torch.Size([101, 3, 160, 160]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_images(directory, t = \".png\"):\n",
    "    \"\"\"\n",
    "    Load images from dir\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): directory to get data\n",
    "    - target_size (tuple): expected size of images without channels\n",
    "\n",
    "    Returns:\n",
    "    - data (torch.Tensor): A torch stack with the data\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # os.walk returns files in arbitrary order\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(t):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_tensor = (\n",
    "                    torch.Tensor(np.array(Image.open(image_path).convert('RGB'), dtype=np.float32)).permute(2,0,1)\n",
    "                    if t == \".tiff\" or t == \".png\"\n",
    "                    else torch.Tensor(colorize_cfa(np.load(image_path), rgb_kf).astype(np.float32)).permute(2,0,1)\n",
    "                )\n",
    "\n",
    "                if image_tensor.max() > 1:\n",
    "                    image_tensor /= 256.0\n",
    "\n",
    "                images.append(image_tensor)\n",
    "    return torch.stack(images)\n",
    "\n",
    "train_clean = load_images(\"/home/gustav/Downloads/ds_proc/none-test\", t=\".png\")\n",
    "train_noise = load_images(\"/home/gustav/Downloads/ds_proc/tiff-test\", t=\".tiff\")\n",
    "train_noise.shape, train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, data_clean, data_noisy):\n",
    "        self.data_clean = data_clean\n",
    "        self.data_noisy = data_noisy\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming both datasets have the same length\n",
    "        return len(self.data_clean)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean_image = self.data_clean[idx]\n",
    "        noisy_image = self.data_noisy[idx]\n",
    "\n",
    "        return clean_image, noisy_image\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True))\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True))\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True))\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True))\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5))\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1), # the 1024 comes from concatenation\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True))\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True))\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True))\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1), \n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.dec1(e5)\n",
    "        d1 = torch.cat((d1, e4), dim=1)  # skip connection\n",
    "        d2 = self.dec2(d1)\n",
    "        d2 = torch.cat((d2, e3), dim=1)  # skip connection\n",
    "        d3 = self.dec3(d2)\n",
    "        d3 = torch.cat((d3, e2), dim=1)  # skip connection\n",
    "        d4 = self.dec4(d3)\n",
    "        d4 = torch.cat((d4, e1), dim=1)  # skip connection\n",
    "        d5 = self.dec5(d4)\n",
    "        return d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 160, 160])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DenoisingAutoencoder()\n",
    "model.forward(train_noise[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_to_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtensor_to_image\u001b[49m(train_clean[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_clean[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_to_image' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(tensor_to_image(train_clean[0]))\n",
    "plt.title(f'Data {train_clean[0].dtype}')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(tensor_to_image(train_noise[0]))\n",
    "plt.title(f'Gussian {train_noise[0].dtype}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/1, Loss: 1.0008522421121597\n"
     ]
    }
   ],
   "source": [
    "def train(train_clean, train_noise, model_dest=\"/\", n_epochs=50):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    paired_dataset = PairedDataset(train_clean, train_noise)\n",
    "    paired_loader  = DataLoader(paired_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    #def ssim_loss(y_true, y_pred):\n",
    "    #    return 1 - ssim(y_true, y_pred, data_range=1, size_average=True)\n",
    "    \n",
    "    model = DenoisingAutoencoder().to(device)\n",
    "    #criterion = ssim_loss\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    print(\"Training\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for clean_images, noisy_images in paired_loader:\n",
    "            clean_images, noisy_images = clean_images.to(device), noisy_images.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(noisy_images)\n",
    "            loss = criterion(outputs, clean_images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "        epoch_loss = running_loss / len(paired_loader)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss}')\n",
    "        \n",
    "    for param in model.parameters():\n",
    "        if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "            print(\"Model contains NaN or inf values\")\n",
    "            \n",
    "    joblib.dump(model, Path(model_dest))\n",
    "\n",
    "train(train_clean, train_noise, model_dest=\"/home/gustav/Downloads/ds_proc/test.pkl\", n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"/home/gustav/Downloads/ds_proc/test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.29571714997291565',\n",
       " '0.33826544880867004',\n",
       " '0.3392800986766815',\n",
       " '0.3564023971557617']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_validation(inputs_clean, inputs_noise, outputs):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 6))\n",
    "    for i in range(3):\n",
    "        for n, (title, data) in enumerate([\n",
    "            (\"Target image\", inputs_clean),\n",
    "            (\"Input noisy\", inputs_noise),\n",
    "            (\"Result image\", outputs)\n",
    "        ]):\n",
    "            axs[n, i].imshow(data[i].permute(1, 2, 0).squeeze())\n",
    "            axs[n, i].title.set_text(title + str(i+1))\n",
    "            axs[n, i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def validate(model, val_clean, val_noise):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    paired_dataset = PairedDataset(val_clean, val_noise)\n",
    "    paired_loader  = DataLoader(paired_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for n, (inputs_clean, inputs_noise) in enumerate(paired_loader):\n",
    "            outputs = model(inputs_noise.to(device))\n",
    "            mse = criterion(outputs, inputs_clean.to(device))\n",
    "            losses.append(mse)\n",
    "    \n",
    "            #if n == 0:\n",
    "            #   show_validation(inputs_clean.cpu(), inputs_noise.cpu(), outputs.cpu())\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = validate(model, train_clean, train_noise)\n",
    "[f\"{loss}\" for loss in losses]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4514338,
     "sourceId": 7726797,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
